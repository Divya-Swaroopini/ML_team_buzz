{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divya-Swaroopini/ML_team_buzz/blob/Divya/ProjectPDPModel1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpHhHK650Clg"
      },
      "outputs": [],
      "source": [
        "#Drive + Unzipping the files into a new folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "'''\n",
        "new_folder_path = '/content/UnZipped'\n",
        "os.mkdir(new_folder_path)'''\n",
        "\n",
        "!unzip gdrive/MyDrive/PlantDiseaseDataset/Apple.zip\n",
        "#!kaggle datasets download -d manjuphoenix/Appledataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qF5soQP0QPG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torch import utils\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "from torch.utils.data import WeightedRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RndcdE80S0C"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on\", device)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh7IoFIQ0W2C"
      },
      "outputs": [],
      "source": [
        "#Update name of dataset\n",
        "from collections import OrderedDict\n",
        "path = '/content/Apple'\n",
        "cls = os.listdir(path)\n",
        "class_map = OrderedDict()\n",
        "for cl in cls:\n",
        "  n = len(os.listdir(path + '/' + cl))\n",
        "  class_map[cl] = n\n",
        "\n",
        "#wj = n(count of imgs in total) / no.of.classes(k) * (count of images of this class j)\n",
        "n = sum(class_map[k] for k in class_map)\n",
        "k = len(class_map)\n",
        "weights = []\n",
        "for cls in class_map.keys():\n",
        "  weights.append(n/((class_map[cls])*k))\n",
        "print(n)\n",
        "print(k)\n",
        "print(class_map)\n",
        "print(weights)\n",
        "'''\n",
        "class_map_ratios = {k: class_map[k]/sum(class_map[k] for k in class_map) for k in class_map}\n",
        "loss_func_weights = []\n",
        "for v in class_map_ratios.values():\n",
        "  loss_func_weights.append(1 - v)\n",
        "print(loss_func_weights)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcWvCb798S3m"
      },
      "outputs": [],
      "source": [
        "'''class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "\n",
        "    def __init__(self, dataset, indices=None, num_samples=None, callback_get_label=None):\n",
        "                \n",
        "        # if indices is not provided, \n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # define custom callback\n",
        "        self.callback_get_label = callback_get_label\n",
        "\n",
        "        # if num_samples is not provided, \n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "            \n",
        "        # distribution of classes in the dataset \n",
        "        label_to_count = {}\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label in label_to_count:\n",
        "                label_to_count[label] += 1\n",
        "            else:\n",
        "                label_to_count[label] = 1\n",
        "                \n",
        "        # weight for each sample\n",
        "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n",
        "                   for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, idx):\n",
        "        return dataset.train_labels[idx].item()\n",
        "                \n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in torch.multinomial(\n",
        "            self.weights, self.num_samples, replacement=True))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdKgYq7d0ZdU"
      },
      "outputs": [],
      "source": [
        "#Transform to Tensor\n",
        "transform =  transforms.Compose(\n",
        "    [transforms.Resize(224),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "#Doubt : Why Exactly do they usually resize the Input to 224x224?\n",
        "#Note : Look up the concept of normalization again to fully understand why this is done\n",
        "data_ = datasets.ImageFolder(path, transform=transform)\n",
        "\n",
        "\n",
        "#Split into 3 subsets -> 30 | 35 | 35\n",
        "size1 = int(len(data_) * 0.3)\n",
        "size2 = int((len(data_) - size1) / 2)\n",
        "size3 = int(len(data_) - (size1 + size2))\n",
        "data1, data2, data3 = utils.data.random_split(data_, [size1, size2, size3])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def balanced_weight_sampler(data):\n",
        "    '''class_weights = []\n",
        "    for root, subdir, files in os.walk(path):\n",
        "      if len(files) > 0:\n",
        "        class_weights.append(1/len(files))\n",
        "\n",
        "    sample_weights = [0] * len(data)\n",
        "    \n",
        "    for idx, (data, label) in enumerate(data):\n",
        "      class_weight = class_weights[label]\n",
        "      sample_weights[idx] = class_weight'''\n",
        "    \n",
        "    sampler = WeightedRandomSampler(weights, num_samples = len(weights), replacement = True)\n",
        "\n",
        "    return sampler"
      ],
      "metadata": {
        "id": "VZ79B4jiVuLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC0pAQD10dJA"
      },
      "outputs": [],
      "source": [
        "def process_and_split__data(data):\n",
        "\n",
        "  testSize = int(len(data) * 0.1)\n",
        "  valSize = int((len(data) - testSize) * 0.2)\n",
        "  trainSize = int((len(data) - (testSize + valSize)))\n",
        "\n",
        "  print(trainSize, testSize, valSize)\n",
        "\n",
        "  train_set, val_set, test_set = utils.data.random_split(data, [trainSize, valSize, testSize])\n",
        "  \n",
        "  train_sampler = balanced_weight_sampler(train_set)\n",
        "  test_sampler = balanced_weight_sampler(test_set)\n",
        "  val_sampler = balanced_weight_sampler(val_set)\n",
        "\n",
        "\n",
        "  '''\n",
        "  y_train_indices = train_set.indices\n",
        "\n",
        "  y_train = [data.targets[i] for i in y_train_indices]\n",
        "\n",
        "  class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n",
        "  weight = 1. / class_sample_count\n",
        "  samples_weight = np.array([weight[t] for t in y_train])\n",
        "  samples_weight = torch.from_numpy(samples_weight)\n",
        "  sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
        "  '''\n",
        "\n",
        "  '''\n",
        "  train_weights = balanced_weights(train_set.images, len(train_set.classes))\n",
        "  test_weights = balanced_weights(train_set.images, len(train_set.classes))\n",
        "  val_weights = balanced_weights(train_set.images, len(train_set.classes))\n",
        "\n",
        "  sampler_train = WeightedRandomSampler(torch.DoubleTensor(train_weights), len(train_weights))\n",
        "  sampler_test = WeightedRandomSampler(torch.DoubleTensor(train_weights), len(train_weights))\n",
        "  sampler_val = WeightedRandomSampler(torch.DoubleTensor(train_weights), len(train_weights))\n",
        "  '''\n",
        "  #Use Dataloader to load the data into an itterable form (Set 1)\n",
        "  train = utils.data.DataLoader(train_set, batch_size = 48, sampler = train_sampler)\n",
        "  test = utils.data.DataLoader(test_set, batch_size = 48, sampler = test_sampler)\n",
        "  val = utils.data.DataLoader(val_set, batch_size = 48, sampler = val_sampler)\n",
        "  \n",
        "  return train, test, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UF9CLaY0eJI"
      },
      "outputs": [],
      "source": [
        "#Comparing DataDistribution between the two classes\n",
        "def compareDataSegments(data1, data2, data3):\n",
        "  data1loader = utils.data.DataLoader(data1, batch_size = 48, shuffle = True)\n",
        "  data2loader = utils.data.DataLoader(data2,  batch_size = 48, shuffle = True)\n",
        "  data3loader = utils.data.DataLoader(data2, batch_size = 48, shuffle = True)\n",
        "  \n",
        "  #Class Names of Numeric Labels\n",
        "  classes = os.listdir(path)\n",
        "  classes.sort()\n",
        "  class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "  print(class_to_idx)\n",
        "\n",
        "  class_count_data1 = DataCount(data1loader)\n",
        "  class_count_data2 = DataCount(data2loader)\n",
        "  class_count_data3 = DataCount(data3loader)\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_axes([0,0,1,1])\n",
        "  ax.set_title('Data Distribution between segments of the Dataset', loc='left')\n",
        "\n",
        "  # Set position of bar on X axis\n",
        "  br1 = np.arange(len(class_count_data1))\n",
        "  br2 = [x + 0.25 for x in br1]\n",
        "  br3 = [x + 0.25 for x in br2]\n",
        "\n",
        "  ax.bar(br1, class_count_data1, color = 'blue', width = 0.25, edgecolor ='black')\n",
        "  ax.bar(br2, class_count_data2, color = 'grey', width = 0.25, edgecolor ='black')\n",
        "  ax.bar(br3, class_count_data3, color = 'pink', width = 0.25, edgecolor ='black')\n",
        "\n",
        "\n",
        "  plt.xlabel('Data Segments', fontweight='bold')\n",
        "  plt.ylabel('Class Count', fontweight='bold')\n",
        "  plt.xticks([r + 0.25 for r in range(len(class_count_data1))], [str(c) for c in range(len(class_count_data1))])\n",
        "    \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuXmmbth0kqq"
      },
      "outputs": [],
      "source": [
        "#Update class_count to the number of classes here\n",
        "\n",
        "#calculating the class distribution over a dataset\n",
        "def DataCount(dataloader):\n",
        "  class_count = np.zeros((4,), dtype=int)\n",
        "  for i, batch in enumerate(dataloader):\n",
        "      image, classes = batch\n",
        "      labels = np.array(classes)\n",
        "      for label in labels:\n",
        "        for i in range(len(classes)):\n",
        "          if(label == i):\n",
        "            class_count[i] = class_count[i] + 1\n",
        "  print(class_count)\n",
        "  return class_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chgbp8Ih0pLh"
      },
      "outputs": [],
      "source": [
        "#Plotting the claculated class distributions\n",
        "def plot_class_distributions(class_count_train, class_count_test):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_axes([0,0,1,1])\n",
        "  ax.set_title('Data Distribution', loc='left')\n",
        "\n",
        "  # Set position of bar on X axis\n",
        "  br1 = np.arange(len(class_count_train))\n",
        "  br2 = [x + 0.25 for x in br1]\n",
        "  br3 = [x + 0.25 for x in br2]\n",
        "\n",
        "  ax.bar(br1, class_count_train, color = 'grey', width = 0.25, edgecolor ='black', label = 'Train')\n",
        "  ax.bar(br2, class_count_test, color = 'pink', width = 0.25, edgecolor ='black', label = 'Test')\n",
        "\n",
        "  plt.xlabel('DataSets', fontweight='bold')\n",
        "  plt.ylabel('Class Count', fontweight='bold')\n",
        "  plt.xticks([r + 0.25 for r in range(len(class_count_train))], [str(c) for c in range(len(class_count_train))])\n",
        "   \n",
        "  plt.show()\n",
        "\n",
        "  #Class Names of Numeric Labels\n",
        "  classNames = os.listdir(path)\n",
        "  classNames.sort()\n",
        "  class_to_idx = {classNames[i]: i for i in range(len(classNames))}\n",
        "  print(class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFWjakuY0p9U"
      },
      "outputs": [],
      "source": [
        "#CURRENTLY NOT IN USE\n",
        "#Class Ratios : Are they as even as expected?\n",
        "def check_class_ratio(class_count_train, class_count_test, classes):\n",
        "  class_ratios = np.zeros(())\n",
        "  #Initialize an empty zeroes array for all 39 classes\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    aggregate = class_count_train[i] + class_count_test[i]\n",
        "    #print(aggregate)\n",
        "    class_ratios[i][0] = round((class_count_train[i] / aggregate) * 100, 2)\n",
        "    class_ratios[i][1] = round((class_count_test[i] / aggregate) * 100, 2)\n",
        "\n",
        "  print(\"Rough Percentage of Class Division amongst the three train and test sets\")\n",
        "  for i in range(len(classes)):\n",
        "    print(class_ratios[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTZV0-mr0slc"
      },
      "outputs": [],
      "source": [
        "#Creating a simple CNN architechture\n",
        "class SimpleCustomCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCustomCNN, self).__init__()\n",
        "\n",
        "    #Defining a sequential model layers\n",
        "    self.c1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 3, out_channels = 8, kernel_size=5, padding=0, stride=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    #self.bn2d = nn.BatchNorm2d(16)\n",
        "    self.c2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size=3, padding=0, stride=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    '''self.c3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=3, padding=0, stride=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )'''\n",
        "    self.drop = nn.Dropout()\n",
        "    self.fc = nn.Linear(in_features = 54*54*16, out_features = 4)\n",
        "    #26*26*32\n",
        "  def forward(self, x):\n",
        "    output = self.c1(x)\n",
        "    #output = self.drop(output)\n",
        "    output = self.c2(output)\n",
        "    #output = self.drop(output)\n",
        "    #output = self.c3(output)\n",
        "    output = output.reshape(output.size(0), -1)\n",
        "    #or nn.Flatten()\n",
        "    output = self.drop(output)\n",
        "    output = self.fc(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0lagdqo0wW_"
      },
      "outputs": [],
      "source": [
        "#Calling all the functions to visualize the Data\n",
        "def datasets_visualization(train, test):\n",
        "\n",
        "  dataiter = iter(train)\n",
        "  images, classes = dataiter.next()\n",
        "\n",
        "  print(type(images))\n",
        "  print(images.shape)\n",
        "  print(classes.shape)\n",
        "\n",
        "  class_count_train = DataCount(train)\n",
        "  class_count_test = DataCount(test)\n",
        "\n",
        "  #Check if the Ratio of the Train:Val:Test has been maintained through the classes:\n",
        "  #check_class_ratio(class_count_train, class_count_test, classes)\n",
        "\n",
        "  #Plot to Visualize the way Data is Distributed between the sets\n",
        "  plot_class_distributions(class_count_train, class_count_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDlr-g0_0ysa"
      },
      "outputs": [],
      "source": [
        "def train_validate_model(e, train, test, val, model, optimizer):\n",
        "\n",
        "  train_losses = []\n",
        "  train_accuracy_list = []\n",
        "\n",
        "  test_losses = []\n",
        "  test_accuracy_list = []\n",
        "\n",
        "  val_losses = []\n",
        "  val_accuracy_list = []\n",
        "\n",
        "  # Initialize the prediction and label lists(tensors)\n",
        "  predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "  lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "\n",
        "  predlist__ =torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "  lbllist__ =torch.zeros(0,dtype=torch.long, device='cpu')\n",
        "\n",
        "  for epoch in range(e):\n",
        "\n",
        "    run_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    t_loss = 0.0\n",
        "\n",
        "    #TRAINING\n",
        "\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for img, class_ in train:\n",
        "      \n",
        "          img, class_ = img.to(device), class_.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          preds = model(img)\n",
        "\n",
        "          loss = lossCriteria(preds, class_)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          run_loss += loss.item()\n",
        "\n",
        "          _, predicted = preds.max(1)\n",
        "          total += class_.size(0)\n",
        "          correct += predicted.eq(class_).sum().item()\n",
        "\n",
        "    \n",
        "    accuracy_train = correct * 100. / total\n",
        "    train_accuracy_list.append(accuracy_train)\n",
        "\n",
        "\n",
        "    train_loss = run_loss / len(train.sampler)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    #validate\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      correct_ = 0\n",
        "      total_ = 0\n",
        "      \n",
        "      for img, class_ in val:\n",
        "\n",
        "          img, class_ = img.to(device), class_.to(device)\n",
        "\n",
        "          preds_ = model(img)\n",
        "          loss = lossCriteria(preds_, class_)\n",
        "          val_loss += loss.item()\n",
        "            \n",
        "          _, predicted = preds_.max(1)\n",
        "          total_ += class_.size(0)\n",
        "          correct_ += predicted.eq(class_).sum().item()\n",
        "\n",
        "          #Append batch prediction results\n",
        "          predlist = torch.cat([predlist, predicted.view(-1).cpu()])\n",
        "          lbllist = torch.cat([lbllist, class_.view(-1).cpu()])\n",
        "\n",
        "    \n",
        "    accuracy_val = correct_ * 100. / total_\n",
        "    val_accuracy_list.append(accuracy_val)\n",
        "\n",
        "\n",
        "    val_loss = val_loss / len(test.sampler)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tTest Loss: {:.4f} \\tValidation Accuracy: {:.4f}'.format(epoch, train_loss, accuracy_train, val_loss, accuracy_val))\n",
        "\n",
        "  #test\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    correct_ = 0\n",
        "    total_ = 0\n",
        "      \n",
        "    for img, class_ in test:\n",
        "\n",
        "        img, class_ = img.to(device), class_.to(device)\n",
        "\n",
        "        preds_ = model(img)\n",
        "        loss = lossCriteria(preds_, class_)\n",
        "        t_loss += loss.item()\n",
        "            \n",
        "        _, predicted = preds_.max(1)\n",
        "        total_ += class_.size(0)\n",
        "        correct_ += predicted.eq(class_).sum().item()\n",
        "        \n",
        "        #Append batch prediction results\n",
        "        predlist__ = torch.cat([predlist, predicted.view(-1).cpu()])\n",
        "        lbllist__ = torch.cat([lbllist, class_.view(-1).cpu()])\n",
        "\n",
        "    accuracy_test = correct_ * 100. / total_\n",
        "    test_accuracy_list.append(accuracy_test)\n",
        "\n",
        "\n",
        "    t_loss = t_loss / len(test.sampler)\n",
        "    test_losses.append(t_loss)\n",
        "  \n",
        "  #Confusion matrix\n",
        "  conf_mat = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
        "  print(\"The Val conf mat \", conf_mat)\n",
        "\n",
        "  #Per-class accuracy\n",
        "  class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "  print(\"Per class validation accuracy \", class_accuracy)\n",
        "\n",
        "  #Confusion matrix\n",
        "  conf_mat = confusion_matrix(lbllist.numpy(), predlist.numpy())\n",
        "  print(\"The Test conf mat \", conf_mat)\n",
        "\n",
        "  #Per-class accuracy\n",
        "  class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
        "  print(\"Per class testing accuracy \", class_accuracy)\n",
        "\n",
        "  #Using sns heatmap to represent\n",
        "  '''\n",
        "  nb_classes = 4\n",
        "  confusion_matrix = np.zeros((nb_classes, nb_classes))\n",
        "  with torch.no_grad():\n",
        "      for i, (inputs, classes) in enumerate(test):\n",
        "          inputs = inputs.to(device)\n",
        "          classes = classes.to(device)\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                  confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "\n",
        "  class_names = list(os.listdir(path))\n",
        "  df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
        "  heatmap = sn.heatmap(df_cm, annot=True, fmt=\"d\")\n",
        "\n",
        "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\n",
        "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  '''\n",
        " \n",
        "\n",
        "  return train_losses, train_accuracy_list, test_losses, test_accuracy_list, val_losses, val_accuracy_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diJphkD504Dn"
      },
      "outputs": [],
      "source": [
        "#Visulaize the Results\n",
        "def plot_acc_curve(train_accuracy, validation_accuracy):\n",
        "\n",
        "  plt.plot(train_accuracy, color='green')\n",
        "  plt.plot(validation_accuracy, color='blue')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training, Validation and Testing Accuracy')\n",
        " \n",
        "  plt.show()\n",
        "\n",
        "def plot_loss_curve(train_losses, validation_losses):\n",
        "  plt.plot(train_losses, color='green')\n",
        "  plt.plot(validation_losses, color='blue')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training, Validation and Testing Losses')\n",
        " \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr4UravJ06ba"
      },
      "outputs": [],
      "source": [
        "#Call the Train and Validate and Test Functions\n",
        "def train_validate_models(model_, train, test, val, optimizer_):\n",
        "  train_losses, train_accuracy, test_losses, test_accuracy, val_losses, val_accuracy = train_validate_model(75, train, test, val, model_, optimizer_)\n",
        "  \n",
        "  plot_loss_curve(train_losses, val_losses)\n",
        "  plot_acc_curve(train_accuracy, val_accuracy)\n",
        "\n",
        "  print('Testing loss', test_losses)\n",
        "  print('Testing Accuracy', test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bq9ejEb08Y6"
      },
      "outputs": [],
      "source": [
        "#Splitting into training and testing datasets\n",
        "train1, test1, val1 = process_and_split__data(data1)\n",
        "train2, test2, val2 = process_and_split__data(data2)\n",
        "train3, test3, val3 = process_and_split__data(data3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABfQU_8Y0-KM"
      },
      "outputs": [],
      "source": [
        "#Cross Entropy Function\n",
        "lossCriteria = nn.CrossEntropyLoss()\n",
        "#weight = torch.tensor(loss_func_weights).to(device)\n",
        "\n",
        "#Initialize model and optimizer 1\n",
        "#Note : 0.01 is actually the default value of the weight decay\n",
        "model1 = SimpleCustomCNN().to(device)\n",
        "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.0005, weight_decay = 0.005)\n",
        "print(model1)\n",
        "#Calling the training function for model 1\n",
        "train_validate_models(model1, train1, test1, val1, optimizer1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n6iEEgQ9COk"
      },
      "outputs": [],
      "source": [
        "#Initialize model and optimizer 2\n",
        "model2 = SimpleCustomCNN().to(device)\n",
        "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "print(model2)\n",
        "#Calling the training function for model 2\n",
        "train_validate_models(model2, train2, test2, val2, optimizer2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SxY3Q099FLv"
      },
      "outputs": [],
      "source": [
        "#Initialize model and optimizer 3\n",
        "model3 = SimpleCustomCNN().to(device)\n",
        "optimizer3 = torch.optim.SGD(model3.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "print(model3)\n",
        "#Calling the training function for model 3\n",
        "train_validate_models(model3, train3, test3, val3, optimizer3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwFPpst5nysV"
      },
      "outputs": [],
      "source": [
        "def visualizations():\n",
        "\n",
        "  #Visualizing the Induvidual Datasets :\n",
        "  print(\"Dataset Segment 1 : \")\n",
        "  datasets_visualization(train1, test1)\n",
        "  print(\"Dataset Segment 2 : \")\n",
        "  datasets_visualization(train2, test2)\n",
        "  print(\"Dataset Segment 3 : \")\n",
        "  datasets_visualization(train3, test3)\n",
        "\n",
        "  #Compare these Data Segemnts\n",
        "  compareDataSegments(data1, data2, data3)\n",
        "\n",
        "visualizations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-ak5RHw1C74"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#Visualize through a confusion matrix\n",
        "\n",
        "#Getting the list of prediction made by model\n",
        "def get_all_preds(model, loader):\n",
        "    all_preds = torch.tensor([])\n",
        "    for batch in loader:\n",
        "        images, labels = batch\n",
        "\n",
        "        preds = model(images)\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "    return all_preds\n",
        "\n",
        "#Counting the total number of correct predictions made\n",
        "def get_num_correct():\n",
        "\n",
        "#Calling above two functions\n",
        "with torch.no_grad():\n",
        "    prediction_loader = torch.utils.data.DataLoader(train1, batch_size=10000)\n",
        "    train1_preds = get_all_preds(network, prediction_loader)\n",
        "preds_correct = get_num_correct(train1_preds, train1.targets)\n",
        "\n",
        "print('Total Correct:', preds_correct)\n",
        "print('Accuracy:', preds_correct / len(train1))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DptuvUDF1DZd"
      },
      "outputs": [],
      "source": [
        "#saving model state:\n",
        "#torch.save(model1.state_dict(), '/content/gdrive/MyDrive/Model_SGD_50_84.97')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP6JxHHV1GU_"
      },
      "outputs": [],
      "source": [
        "#Load Previously saved model\n",
        "'''\n",
        "modelOld =  SimpleCustomCNN().to(device)\n",
        "modelOld.load_state_dict = torch.load('/content/gdrive/MyDrive/CustomCNN_93.4911_apple')\n",
        "print(modelOld)\n",
        "modelOld.to(device)\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ProjectPDPModel1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPYYM9YuqRye9B2tUBNtHoO",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}