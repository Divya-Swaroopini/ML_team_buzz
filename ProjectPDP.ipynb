{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Divya-Swaroopini/ML_team_buzz/blob/main/ProjectPDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpHhHK650Clg"
      },
      "outputs": [],
      "source": [
        "#Drive + Unzipping the files into a new folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!unzip gdrive/MyDrive/PlantDiseaseDataset/Apple.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63HEJ9VM8Ucs"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qF5soQP0QPG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torch import utils\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RndcdE80S0C"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Running on\", device)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh7IoFIQ0W2C"
      },
      "outputs": [],
      "source": [
        "#Update name of dataset\n",
        "\n",
        "path = '/content/Apple'\n",
        "cls = os.listdir(path)\n",
        "class_map = {}\n",
        "for cl in cls:\n",
        "  n = len(os.listdir(path + '/' + cl))\n",
        "  class_map[cl] = n\n",
        "print(class_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdKgYq7d0ZdU"
      },
      "outputs": [],
      "source": [
        "#Transform to Tensor\n",
        "transform =  transforms.Compose(\n",
        "    [transforms.Resize(224),\n",
        "     transforms.ToTensor()])\n",
        "#Doubt : Why Exactly do they usually resize the Input to 224x224?\n",
        "#Note : Look up the concept of normalization again to fully understand why this is done\n",
        "data_ = datasets.ImageFolder(path, transform=transform)\n",
        "\n",
        "#Split into 3 subsets -> 30 | 35 | 35\n",
        "size1 = int(len(data_) * 0.3)\n",
        "size2 = int((len(data_) - size1) / 2)\n",
        "size3 = int(len(data_) - (size1 + size2))\n",
        "data1, data2, data3 = utils.data.random_split(data_, [size1, size2, size3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC0pAQD10dJA"
      },
      "outputs": [],
      "source": [
        "def process_and_split__data(data):\n",
        "\n",
        "  testSize = int(len(data) * 0.3)\n",
        "  trainSize = len(data) - testSize\n",
        "  train_set, test_set = utils.data.random_split(data, [trainSize, testSize])\n",
        "\n",
        "  #Use Dataloader to load the data into an itterable form (Set 1)\n",
        "  train = utils.data.DataLoader(train_set, batch_size = 32, shuffle = True)\n",
        "  test = utils.data.DataLoader(test_set, batch_size = 32, shuffle = True)\n",
        "  \n",
        "  return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UF9CLaY0eJI"
      },
      "outputs": [],
      "source": [
        "#Comparing DataDistribution between the two classes\n",
        "def compareDataSegments(data1, data2, data3):\n",
        "  data1loader = utils.data.DataLoader(data1, batch_size = 32, shuffle = True)\n",
        "  data2loader = utils.data.DataLoader(data2, batch_size = 32, shuffle = True)\n",
        "  data3loader = utils.data.DataLoader(data2, batch_size = 32, shuffle = True)\n",
        "  \n",
        "  #Class Names of Numeric Labels\n",
        "  classes = os.listdir(path)\n",
        "  classes.sort()\n",
        "  class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "  print(class_to_idx)\n",
        "\n",
        "  class_count_data1 = DataCount(data1loader)\n",
        "  class_count_data2 = DataCount(data2loader)\n",
        "  class_count_data3 = DataCount(data3loader)\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_axes([0,0,1,1])\n",
        "  ax.set_title('Data Distribution between segments of the Dataset', loc='left')\n",
        "\n",
        "  # Set position of bar on X axis\n",
        "  br1 = np.arange(len(class_count_data1))\n",
        "  br2 = [x + 0.25 for x in br1]\n",
        "  br3 = [x + 0.25 for x in br2]\n",
        "\n",
        "  ax.bar(br1, class_count_data1, color = 'blue', width = 0.25, edgecolor ='black')\n",
        "  ax.bar(br2, class_count_data2, color = 'grey', width = 0.25, edgecolor ='black')\n",
        "  ax.bar(br3, class_count_data3, color = 'pink', width = 0.25, edgecolor ='black')\n",
        "\n",
        "\n",
        "  plt.xlabel('Data Segments', fontweight='bold')\n",
        "  plt.ylabel('Class Count', fontweight='bold')\n",
        "  plt.xticks([r + 0.25 for r in range(len(class_count_data1))], [str(c) for c in range(len(class_count_data1))])\n",
        "    \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuXmmbth0kqq"
      },
      "outputs": [],
      "source": [
        "#Update class_count to the number of classes here\n",
        "\n",
        "#calculating the class distribution over a dataset\n",
        "def DataCount(dataloader):\n",
        "  class_count = np.zeros((4,), dtype=int)\n",
        "  for i, batch in enumerate(dataloader):\n",
        "      image, classes = batch\n",
        "      labels = np.array(classes)\n",
        "      for label in labels:\n",
        "        for i in range(len(classes)):\n",
        "          if(label == i):\n",
        "            class_count[i] = class_count[i] + 1\n",
        "  print(class_count)\n",
        "  return class_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chgbp8Ih0pLh"
      },
      "outputs": [],
      "source": [
        "#Plotting the claculated class distributions\n",
        "def plot_class_distributions(class_count_train, class_count_test):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_axes([0,0,1,1])\n",
        "  ax.set_title('Data Distribution', loc='left')\n",
        "\n",
        "  # Set position of bar on X axis\n",
        "  br1 = np.arange(len(class_count_train))\n",
        "  br2 = [x + 0.25 for x in br1]\n",
        "  br3 = [x + 0.25 for x in br2]\n",
        "\n",
        "  ax.bar(br1, class_count_train, color = 'grey', width = 0.25, edgecolor ='black', label = 'Train')\n",
        "  ax.bar(br2, class_count_test, color = 'pink', width = 0.25, edgecolor ='black', label = 'Test')\n",
        "\n",
        "  plt.xlabel('DataSets', fontweight='bold')\n",
        "  plt.ylabel('Class Count', fontweight='bold')\n",
        "  plt.xticks([r + 0.25 for r in range(len(class_count_train))], [str(c) for c in range(len(class_count_train))])\n",
        "   \n",
        "  plt.show()\n",
        "\n",
        "  #Class Names of Numeric Labels\n",
        "  classNames = os.listdir(path)\n",
        "  classNames.sort()\n",
        "  class_to_idx = {classNames[i]: i for i in range(len(classNames))}\n",
        "  print(class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFWjakuY0p9U"
      },
      "outputs": [],
      "source": [
        "#CURRENTLY NOT IN USE\n",
        "#Class Ratios : Are they as even as expected?\n",
        "def check_class_ratio(class_count_train, class_count_test, classes):\n",
        "  class_ratios = np.zeros(())\n",
        "  #Initialize an empty zeroes array for all 39 classes\n",
        "\n",
        "  for i in range(len(classes)):\n",
        "    aggregate = class_count_train[i] + class_count_test[i]\n",
        "    #print(aggregate)\n",
        "    class_ratios[i][0] = round((class_count_train[i] / aggregate) * 100, 2)\n",
        "    class_ratios[i][1] = round((class_count_test[i] / aggregate) * 100, 2)\n",
        "\n",
        "  print(\"Rough Percentage of Class Division amongst the three train and test sets\")\n",
        "  for i in range(len(classes)):\n",
        "    print(class_ratios[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTZV0-mr0slc"
      },
      "outputs": [],
      "source": [
        "#Creating a simple CNN architechture\n",
        "class SimpleCustomCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCustomCNN, self).__init__()\n",
        "\n",
        "    #Defining a sequential model layers\n",
        "    self.c1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size=5, padding=0, stride=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.c2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=3, padding=0, stride=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.drop = nn.Dropout()\n",
        "    self.fc1 = nn.Linear(in_features = 54*54*32, out_features = 4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.c1(x)\n",
        "    output = self.c2(output)\n",
        "    output = output.reshape(output.size(0), -1)\n",
        "    #or nn.Flatten()\n",
        "    #To prevent over fitting\n",
        "    output = self.drop(output)\n",
        "    output = self.fc1(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0lagdqo0wW_"
      },
      "outputs": [],
      "source": [
        "#Calling all the functions to visualize the Data\n",
        "def datasets_visualization(train, test):\n",
        "\n",
        "  dataiter = iter(train)\n",
        "  images, classes = dataiter.next()\n",
        "\n",
        "  print(type(images))\n",
        "  print(images.shape)\n",
        "  print(classes.shape)\n",
        "\n",
        "  class_count_train = DataCount(train)\n",
        "  class_count_test = DataCount(test)\n",
        "\n",
        "  #Check if the Ratio of the Train:Val:Test has been maintained through the classes:\n",
        "  #check_class_ratio(class_count_train, class_count_test, classes)\n",
        "\n",
        "  #Plot to Visualize the way Data is Distributed between the sets\n",
        "  plot_class_distributions(class_count_train, class_count_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDlr-g0_0ysa"
      },
      "outputs": [],
      "source": [
        "def train_test_model(e, train, test, model, optimizer):\n",
        "\n",
        "  train_losses = []\n",
        "  train_accuracy_list = []\n",
        "\n",
        "  test_losses = []\n",
        "  test_accuracy_list = []\n",
        "\n",
        "\n",
        "  for epoch in range(e):\n",
        "\n",
        "    run_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    #TRAINING\n",
        "\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for img, class_ in train:\n",
        "      \n",
        "          img, class_ = img.to(device), class_.to(device)\n",
        "  \n",
        "          #flood = (loss-b).abs()+b\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          preds = model(img)\n",
        "\n",
        "          loss = lossCriteria(preds, class_)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          run_loss += loss.item()\n",
        "\n",
        "          _, predicted = preds.max(1)\n",
        "          total += class_.size(0)\n",
        "          correct += predicted.eq(class_).sum().item()\n",
        "\n",
        "    \n",
        "    accuracy_train = correct * 100. / total\n",
        "    train_accuracy_list.append(accuracy_train)\n",
        "\n",
        "\n",
        "    train_loss = run_loss / len(train.sampler)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    #TESTING\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      correct_ = 0\n",
        "      total_ = 0\n",
        "      \n",
        "      for img, class_ in test:\n",
        "\n",
        "          img, class_ = img.to(device), class_.to(device)\n",
        "\n",
        "          preds_ = model(img)\n",
        "          loss = lossCriteria(preds_, class_)\n",
        "          val_loss += loss.item()\n",
        "            \n",
        "          _, predicted = preds_.max(1)\n",
        "          total_ += class_.size(0)\n",
        "          correct_ += predicted.eq(class_).sum().item()\n",
        "    \n",
        "    accuracy_test = correct_ * 100. / total_\n",
        "    test_accuracy_list.append(accuracy_test)\n",
        "\n",
        "\n",
        "    test_loss = val_loss / len(test.sampler)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.4f} \\tTraining Accuracy: {:.4f} \\tTest Loss: {:.4f} \\tTest Accuracy: {:.4f}'.format(epoch, train_loss, accuracy_train, test_loss, accuracy_test))\n",
        "\n",
        "  return train_losses, train_accuracy_list, test_losses, test_accuracy_list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ta7xy7tSbjIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diJphkD504Dn"
      },
      "outputs": [],
      "source": [
        "#Visulaize the Results\n",
        "def plot_acc_curve(train_accuracy, test_accuracy):\n",
        "  plt.plot(train_accuracy, color='green')\n",
        "  plt.plot(test_accuracy, color='blue')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training and Testing Accuracy')\n",
        " \n",
        "  plt.show()\n",
        "\n",
        "def plot_loss_curve(train_losses, test_losses):\n",
        "  plt.plot(train_losses, color='green')\n",
        "  plt.plot(test_losses, color='blue')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Testing Losses')\n",
        " \n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr4UravJ06ba"
      },
      "outputs": [],
      "source": [
        "#Call the Train and Validate and Test Functions\n",
        "def train_test_models(model_, train, test, optimizer_):\n",
        "  train_losses, train_accuracy, test_losses, test_accuracy = train_test_model(100, train, test, model_, optimizer_)\n",
        "\n",
        "  plot_loss_curve(train_losses, test_losses)\n",
        "  plot_acc_curve(train_accuracy, test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bq9ejEb08Y6"
      },
      "outputs": [],
      "source": [
        "#Splitting into training and testing datasets\n",
        "train1, test1 = process_and_split__data(data1)\n",
        "train2, test2 = process_and_split__data(data2)\n",
        "train3, test3 = process_and_split__data(data3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABfQU_8Y0-KM"
      },
      "outputs": [],
      "source": [
        "#Cross Entropy Function\n",
        "lossCriteria = nn.CrossEntropyLoss()\n",
        "\n",
        "#Initialize model and optimizer 1\n",
        "#Note : 0.01 is actually the default value of the weight decay\n",
        "model1 = SimpleCustomCNN().to(device)\n",
        "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "print(model1)\n",
        "#Calling the training function for model 1\n",
        "train_test_models(model1, train1, test1, optimizer1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_n6iEEgQ9COk"
      },
      "outputs": [],
      "source": [
        "#Initialize model and optimizer 2\n",
        "model2 = SimpleCustomCNN().to(device)\n",
        "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "print(model2)\n",
        "#Calling the training function for model 2\n",
        "train_test_models(model2, train2, test2, optimizer2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SxY3Q099FLv"
      },
      "outputs": [],
      "source": [
        "#Initialize model and optimizer 3\n",
        "model3 = SimpleCustomCNN().to(device)\n",
        "optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "print(model3)\n",
        "#Calling the training function for model 3\n",
        "train_test_models(model3, train3, test3, optimizer3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwFPpst5nysV"
      },
      "outputs": [],
      "source": [
        "def visualizations():\n",
        "\n",
        "  #Visualizing the Induvidual Datasets :\n",
        "  print(\"Dataset Segment 1 : \")\n",
        "  datasets_visualization(train1, test1)\n",
        "  print(\"Dataset Segment 2 : \")\n",
        "  datasets_visualization(train2, test2)\n",
        "  print(\"Dataset Segment 3 : \")\n",
        "  datasets_visualization(train3, test3)\n",
        "\n",
        "  #Compare these Data Segemnts\n",
        "  compareDataSegments(data1, data2, data3)\n",
        "\n",
        "visualizations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-ak5RHw1C74"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#Visualize through a confusion matrix\n",
        "\n",
        "#Getting the list of prediction made by model\n",
        "def get_all_preds(model, loader):\n",
        "    all_preds = torch.tensor([])\n",
        "    for batch in loader:\n",
        "        images, labels = batch\n",
        "\n",
        "        preds = model(images)\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "    return all_preds\n",
        "\n",
        "#Counting the total number of correct predictions made\n",
        "def get_num_correct():\n",
        "\n",
        "#Calling above two functions\n",
        "with torch.no_grad():\n",
        "    prediction_loader = torch.utils.data.DataLoader(train1, batch_size=10000)\n",
        "    train1_preds = get_all_preds(network, prediction_loader)\n",
        "preds_correct = get_num_correct(train1_preds, train1.targets)\n",
        "\n",
        "print('Total Correct:', preds_correct)\n",
        "print('Accuracy:', preds_correct / len(train1))'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DptuvUDF1DZd"
      },
      "outputs": [],
      "source": [
        "#saving model state:\n",
        "#torch.save(model.state_dict(), '/content/gdrive/MyDrive/CustomCNN_93.4911_apple')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP6JxHHV1GU_"
      },
      "outputs": [],
      "source": [
        "#Load Previously saved model\n",
        "'''\n",
        "modelOld =  SimpleCustomCNN().to(device)\n",
        "modelOld.load_state_dict = torch.load('/content/gdrive/MyDrive/CustomCNN_93.4911_apple')\n",
        "print(modelOld)\n",
        "modelOld.to(device)\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ProjectPDP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNRdXEXKH0QBfpbd2R0o1th",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}